"""
å¿«é€Ÿæµ‹è¯•è„šæœ¬ - æ‰¹é‡æµ‹è¯•å¤šå¼ å›¾ç‰‡ï¼Œå¯¹æ¯”ä¿®å¤å‰åçš„å·®å¼‚

ä½¿ç”¨æ–¹æ³•ï¼š
    python quick_test.py --folder "æµ‹è¯•å›¾ç‰‡æ–‡ä»¶å¤¹è·¯å¾„"
    
æˆ–è€…æµ‹è¯•å•å¼ å›¾ç‰‡ï¼š
    python quick_test.py --image "æµ‹è¯•å›¾ç‰‡.jpg"
"""

import os
import torch
from torchvision import transforms
from PIL import Image
import argparse
from pathlib import Path
from models.mobilenet_v2 import get_model
import config
import json
from utils.misc import safe_torch_load

# ç¡®ä¿ç±»åˆ«åå·²åŠ è½½
def ensure_class_names():
    if config.CLASS_NAMES is None:
        json_path = os.path.join(config.SPLITS_DIR, "class_names.json")
        if os.path.isfile(json_path):
            with open(json_path, "r", encoding="utf-8") as f:
                config.CLASS_NAMES = json.load(f)
        else:
            from torchvision import datasets
            full_dataset = datasets.ImageFolder(root=config.DATA_ROOT)
            config.CLASS_NAMES = full_dataset.classes


def get_old_transform():
    """æ—§çš„é¢„å¤„ç†æ–¹å¼ï¼ˆé”™è¯¯çš„ï¼‰"""
    return transforms.Compose([
        transforms.Resize((config.IMAGE_SIZE, config.IMAGE_SIZE)),
        transforms.ToTensor(),
        transforms.Normalize(
            mean=[0.485, 0.456, 0.406],
            std=[0.229, 0.224, 0.225]
        )
    ])


def get_new_transform():
    """æ–°çš„é¢„å¤„ç†æ–¹å¼ï¼ˆæ­£ç¡®çš„ï¼‰"""
    return transforms.Compose([
        transforms.Resize(int(config.IMAGE_SIZE * 1.14)),  # 256
        transforms.CenterCrop(config.IMAGE_SIZE),           # 224
        transforms.ToTensor(),
        transforms.Normalize(
            mean=[0.485, 0.456, 0.406],
            std=[0.229, 0.224, 0.225]
        )
    ])


def predict_with_transform(image_path, model, transform, top_k=3):
    """ä½¿ç”¨æŒ‡å®šçš„transformè¿›è¡Œé¢„æµ‹"""
    ensure_class_names()
    
    # åŠ è½½å¹¶é¢„å¤„ç†å›¾åƒ
    image = Image.open(image_path).convert("RGB")
    image_tensor = transform(image).unsqueeze(0).to(config.DEVICE)
    
    # é¢„æµ‹
    model.eval()
    with torch.no_grad():
        outputs = model(image_tensor)
        probabilities = torch.nn.functional.softmax(outputs, dim=1)
        top_probs, top_indices = torch.topk(probabilities[0], min(top_k, len(config.CLASS_NAMES)))
        
        results = []
        for prob, idx in zip(top_probs, top_indices):
            results.append({
                'class': config.CLASS_NAMES[idx.item()],
                'confidence': prob.item() * 100
            })
    
    return results


def compare_predictions(image_path, model):
    """å¯¹æ¯”æ–°æ—§é¢„å¤„ç†æ–¹å¼çš„é¢„æµ‹ç»“æœ"""
    old_transform = get_old_transform()
    new_transform = get_new_transform()
    
    old_results = predict_with_transform(image_path, model, old_transform, top_k=3)
    new_results = predict_with_transform(image_path, model, new_transform, top_k=3)
    
    return {
        'image': os.path.basename(image_path),
        'old_method': old_results,
        'new_method': new_results
    }


def print_comparison(result):
    """æ‰“å°å¯¹æ¯”ç»“æœ"""
    print("\n" + "="*80)
    print(f"ğŸ“· å›¾ç‰‡ï¼š{result['image']}")
    print("="*80)
    
    print("\nâŒ æ—§æ–¹æ³•ï¼ˆResize(224,224) - é”™è¯¯ï¼‰:")
    for i, pred in enumerate(result['old_method'], 1):
        marker = "â˜…" if i == 1 else " "
        warning = " âš ï¸ å¯èƒ½é”™è¯¯" if pred['class'] == "Background_without_leaves" else ""
        print(f"{marker} {i}. {pred['class']:<40} {pred['confidence']:6.2f}%{warning}")
    
    print("\nâœ… æ–°æ–¹æ³•ï¼ˆResize(256) + CenterCrop(224) - æ­£ç¡®ï¼‰:")
    for i, pred in enumerate(result['new_method'], 1):
        marker = "â˜…" if i == 1 else " "
        warning = " âš ï¸ å¯èƒ½é”™è¯¯" if pred['class'] == "Background_without_leaves" else ""
        print(f"{marker} {i}. {pred['class']:<40} {pred['confidence']:6.2f}%{warning}")
    
    # åˆ†æå·®å¼‚
    old_top1 = result['old_method'][0]
    new_top1 = result['new_method'][0]
    
    if old_top1['class'] != new_top1['class']:
        print(f"\nğŸ”„ é¢„æµ‹å‘ç”Ÿå˜åŒ–ï¼š{old_top1['class']} â†’ {new_top1['class']}")
        conf_diff = new_top1['confidence'] - old_top1['confidence']
        print(f"   ç½®ä¿¡åº¦å˜åŒ–ï¼š{old_top1['confidence']:.2f}% â†’ {new_top1['confidence']:.2f}% ({conf_diff:+.2f}%)")
    else:
        print(f"\nâœ”ï¸ é¢„æµ‹ä¸€è‡´ï¼š{old_top1['class']}")
        conf_diff = new_top1['confidence'] - old_top1['confidence']
        if abs(conf_diff) > 1:
            print(f"   ç½®ä¿¡åº¦å˜åŒ–ï¼š{old_top1['confidence']:.2f}% â†’ {new_top1['confidence']:.2f}% ({conf_diff:+.2f}%)")


def test_folder(folder_path, model, max_images=10):
    """æ‰¹é‡æµ‹è¯•æ–‡ä»¶å¤¹ä¸­çš„å›¾ç‰‡"""
    folder = Path(folder_path)
    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.JPG', '.JPEG', '.PNG', '.BMP'}
    
    # æŸ¥æ‰¾æ‰€æœ‰å›¾ç‰‡
    images = [f for f in folder.glob('*') if f.suffix in image_extensions]
    
    if not images:
        print(f"âš ï¸ æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡ï¼š{folder_path}")
        return
    
    # é™åˆ¶æµ‹è¯•æ•°é‡
    images = images[:max_images]
    print(f"\nğŸ” æ‰¾åˆ° {len(images)} å¼ å›¾ç‰‡ï¼Œå¼€å§‹æµ‹è¯•...\n")
    
    # ç»Ÿè®¡
    changed_count = 0
    improved_count = 0
    
    for image_path in images:
        result = compare_predictions(str(image_path), model)
        print_comparison(result)
        
        old_top1 = result['old_method'][0]
        new_top1 = result['new_method'][0]
        
        if old_top1['class'] != new_top1['class']:
            changed_count += 1
            # å¦‚æœæ—§æ–¹æ³•é¢„æµ‹ä¸ºBackground_without_leavesï¼Œæ–°æ–¹æ³•ä¸æ˜¯ï¼Œåˆ™è®¤ä¸ºæ˜¯æ”¹è¿›
            if old_top1['class'] == "Background_without_leaves" and new_top1['class'] != "Background_without_leaves":
                improved_count += 1
    
    # æ‰“å°æ€»ç»“
    print("\n" + "="*80)
    print("ğŸ“Š æµ‹è¯•æ€»ç»“")
    print("="*80)
    print(f"æ€»æµ‹è¯•å›¾ç‰‡æ•°ï¼š{len(images)}")
    print(f"é¢„æµ‹å‘ç”Ÿå˜åŒ–ï¼š{changed_count} å¼  ({changed_count/len(images)*100:.1f}%)")
    print(f"ä»Backgroundä¿®æ­£ï¼š{improved_count} å¼ ")
    print("\nğŸ’¡ ç»“è®ºï¼š")
    if changed_count > 0:
        print("  ä¿®å¤é¢„å¤„ç†æ–¹å¼åï¼Œé¢„æµ‹ç»“æœæœ‰æ˜æ˜¾å˜åŒ–ï¼")
        if improved_count > 0:
            print(f"  å…¶ä¸­ {improved_count} å¼ ä»'æ— å¶ç‰‡èƒŒæ™¯'ä¿®æ­£ä¸ºå…¶ä»–ç±»åˆ«ï¼Œè¿™å¾ˆå¯èƒ½æ˜¯æ”¹è¿›ï¼")
    else:
        print("  é¢„æµ‹ç»“æœæ²¡æœ‰å˜åŒ–ï¼Œè¯´æ˜å½“å‰æµ‹è¯•å›¾ç‰‡ä¸å—é¢„å¤„ç†æ–¹å¼å½±å“ã€‚")
    print("="*80)


def test_single_image(image_path, model):
    """æµ‹è¯•å•å¼ å›¾ç‰‡"""
    if not os.path.isfile(image_path):
        print(f"âŒ å›¾ç‰‡ä¸å­˜åœ¨ï¼š{image_path}")
        return
    
    result = compare_predictions(image_path, model)
    print_comparison(result)


if __name__ == "__main__":
    # å‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description="å¿«é€Ÿæµ‹è¯•è„šæœ¬ - å¯¹æ¯”æ–°æ—§é¢„å¤„ç†æ–¹å¼")
    parser.add_argument("--folder", "-f", type=str, help="æµ‹è¯•å›¾ç‰‡æ–‡ä»¶å¤¹è·¯å¾„")
    parser.add_argument("--image", "-i", type=str, help="æµ‹è¯•å•å¼ å›¾ç‰‡è·¯å¾„")
    parser.add_argument("--max", "-m", type=int, default=10, help="æœ€å¤§æµ‹è¯•å›¾ç‰‡æ•°ï¼ˆé»˜è®¤10ï¼‰")
    args = parser.parse_args()
    
    # ç¡®ä¿ç±»åˆ«åå·²åŠ è½½
    ensure_class_names()
    
    # åŠ è½½æ¨¡å‹
    print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
    model = get_model()
    if not os.path.isfile(config.MODEL_SAVE_PATH):
        print(f"âŒ æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ï¼š{config.MODEL_SAVE_PATH}")
        print("   è¯·å…ˆè¿è¡Œ train.py è®­ç»ƒæ¨¡å‹")
        exit(1)
    
    state_dict = safe_torch_load(
        config.MODEL_SAVE_PATH,
        map_location=config.DEVICE,
    )
    model.load_state_dict(state_dict)
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ\n")
    
    # æ‰§è¡Œæµ‹è¯•
    if args.folder:
        test_folder(args.folder, model, max_images=args.max)
    elif args.image:
        test_single_image(args.image, model)
    else:
        print("âŒ è¯·æŒ‡å®šæµ‹è¯•å›¾ç‰‡æˆ–æ–‡ä»¶å¤¹ï¼š")
        print("   python quick_test.py --folder 'å›¾ç‰‡æ–‡ä»¶å¤¹'")
        print("   python quick_test.py --image 'å›¾ç‰‡.jpg'")

